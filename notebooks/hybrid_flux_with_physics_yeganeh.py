# -*- coding: utf-8 -*-
"""hybrid_flux_with_physics_Yeganeh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GSyluwAKZgEeDNTcbuv-fEdZD2PVRrvi
"""

# @title Imports and setup

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
import math, time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Torch:", torch.__version__)
print("Device:", device)

"""Our 1D electrostatic model is the standard fluid–Poisson reduction of the Vlasov–Poisson system with a cold-fluid closure, as found in classical plasma texts (e.g. Nicholson [1]) and in time-dependent 1D fluid–Poisson simulations of discharges by Loffhagen *et al.* [2] We further add a small viscous term $$( \nu \partial_x^2 u ) $$ to the momentum equation, in analogy with Navier–Stokes regularization.

[1] Introduction to Plasma Theory, https://pukhov.tp1.hhu.de/resources/Lectures/TP/IntroductiontoPlasmaTheory1.pdf

[2] Modeling of Atmospheric-Pressure Dielectric Barrier Discharges in Argon with Small Admixtures of Tetramethylsilane, Journal of Plasma Chemistry and Plasma Processing https://link.springer.com/article/10.1007/s11090-020-10121-y
"""

# @title Cell 2 Fluid–Poisson BaselineSolver (with u advection)

# class BaselineSolver:
#     def __init__(self, nx=64, length=2*math.pi, dt=5e-3, t_end=1.0):
#         """
#         1D fluid–Poisson toy model:
#           - dn/dt + d(nu)/dx = 0
#           - du/dt + d(u^2/2)/dx = E
#           - dE/dx = n - n0 (n0 = 1)
#         """
#         self.nx = nx
#         self.length = length
#         self.dx = length / nx
#         self.dt = dt
#         self.t_end = t_end
#         self.x = np.linspace(0.5*self.dx, length - 0.5*self.dx, nx)
#         self.n0 = 1.0

#         if self.dt / self.dx > 0.5:
#             print("Warning: dt/dx may be large; consider reducing dt for stability.")

#         # FFT wavenumbers for Poisson solve
#         k = 2.0 * np.pi * np.fft.fftfreq(nx, d=self.dx)
#         self.k = k

#     def initial_condition(self, seed=None):
#         """
#         Density: background + sum of sinusoidal modes.
#         Velocity: small random perturbation.
#         """
#         rng = np.random.RandomState(seed)
#         n = np.full(self.nx, self.n0, dtype=np.float32)
#         for _ in range(2):
#             k_mode = rng.randint(1, 4)
#             amp = 0.1 * rng.rand()
#             phase = 2*np.pi*rng.rand()
#             n += amp * np.sin(k_mode * self.x + phase).astype(np.float32)

#         u = 0.1 * rng.randn(self.nx).astype(np.float32)
#         E = self.solve_poisson(n)
#         state = np.stack([n.astype(np.float32),
#                           u.astype(np.float32),
#                           E.astype(np.float32)], axis=0)  # [3, nx]
#         return state

#     def solve_poisson(self, n):
#         """
#         Solve dE/dx = n - n0 with periodic BC using FFT.
#         """
#         rho = n - self.n0
#         rho_hat = np.fft.fft(rho)
#         k = self.k
#         E_hat = np.zeros_like(rho_hat, dtype=complex)
#         mask = k != 0
#         E_hat[mask] = 1j * rho_hat[mask] / k[mask]
#         E_hat[~mask] = 0.0
#         E = np.real(np.fft.ifft(E_hat)).astype(np.float32)
#         return E

#     def compute_flux_n(self, n, u):
#         """Continuity flux F_n = n * u."""
#         return (n * u).astype(np.float32)

#     def compute_flux_u(self, u):
#         """Momentum flux F_u = u^2 / 2."""
#         return (0.5 * u * u).astype(np.float32)

#     def step(self, state):
#         """
#         One explicit time step:
#           state: [3, nx] with (n, u, E)
#         """
#         n, u, E = state

#         # continuity
#         F_n = self.compute_flux_n(n, u)
#         F_n_left = np.roll(F_n, 1)
#         n_new = n - (self.dt/self.dx) * (F_n - F_n_left)

#         # momentum: u advection + E forcing
#         F_u = self.compute_flux_u(u)
#         F_u_left = np.roll(F_u, 1)
#         u_adv = u - (self.dt/self.dx) * (F_u - F_u_left)
#         u_new = u_adv + self.dt * E

#         # field from new density
#         E_new = self.solve_poisson(n_new)

#         state_new = np.stack([n_new, u_new, E_new], axis=0).astype(np.float32)
#         return state_new, F_n  # continuity flux is what GNN will learn

#     def run(self, state0, n_steps=10, record_flux=True):
#         states = [state0.astype(np.float32)]
#         fluxes = []
#         state = state0.astype(np.float32)
#         for _ in range(n_steps):
#             state, F_n = self.step(state)
#             states.append(state)
#             if record_flux:
#                 fluxes.append(F_n)
#         states = np.stack(states, axis=0)  # [T+1, 3, nx]
#         if record_flux:
#             fluxes = np.stack(fluxes, axis=0)  # [T, nx]
#         else:
#             fluxes = None
#         return states, fluxes

# # smoke test
# solver = BaselineSolver()
# s0 = solver.initial_condition(seed=0)
# states_smoke, fluxes_smoke = solver.run(s0, n_steps=5)
# print("Baseline states:", states_smoke.shape, "fluxes:", fluxes_smoke.shape)


################################################################

class BaselineSolver:
    def __init__(self, nx=64, length=2*math.pi, dt=5e-3, t_end=1.0, nu=1e-3):
        """
        1D fluid–Poisson model with viscosity:
          dn/dt + d(nu)/dx = 0
          du/dt + d(u^2/2)/dx = E + nu * d^2u/dx^2
          dE/dx = n - n0 (n0 = 1)
        """
        self.nx = nx
        self.length = length
        self.dx = length / nx
        self.dt = dt
        self.t_end = t_end
        self.x = np.linspace(0.5*self.dx, length - 0.5*self.dx, nx)
        self.n0 = 1.0
        self.nu = nu

        if self.dt / self.dx > 0.5:
            print("Warning: dt/dx may be large; consider reducing dt for stability.")

        k = 2.0 * np.pi * np.fft.fftfreq(nx, d=self.dx)
        self.k = k

    def initial_condition(self, seed=None):
        """
        Richer IC:
          - density: background + 3–5 sine modes with larger amplitudes
          - velocity: 2 cosine modes + small noise
        """
        rng = np.random.RandomState(seed)
        n = np.full(self.nx, self.n0, dtype=np.float32)

        num_modes = rng.randint(3, 6)  # 3–5 modes
        for _ in range(num_modes):
            k_mode = rng.randint(1, 6)
            amp = 0.15 + 0.15 * rng.rand()  # up to ~0.3
            phase = 2*np.pi*rng.rand()
            n += amp * np.sin(k_mode * self.x + phase).astype(np.float32)

        u = np.zeros(self.nx, dtype=np.float32)
        for _ in range(2):
            k_mode = rng.randint(1, 6)
            amp = 0.1 + 0.1 * rng.rand()
            phase = 2*np.pi*rng.rand()
            u += amp * np.cos(k_mode * self.x + phase).astype(np.float32)
        u += 0.05 * rng.randn(self.nx).astype(np.float32)

        E = self.solve_poisson(n)
        state = np.stack([n.astype(np.float32),
                          u.astype(np.float32),
                          E.astype(np.float32)], axis=0)
        return state

    def solve_poisson(self, n):
        rho = n - self.n0
        rho_hat = np.fft.fft(rho)
        k = self.k
        E_hat = np.zeros_like(rho_hat, dtype=complex)
        mask = k != 0
        E_hat[mask] = 1j * rho_hat[mask] / k[mask]
        E_hat[~mask] = 0.0
        E = np.real(np.fft.ifft(E_hat)).astype(np.float32)
        return E

    def compute_flux_n(self, n, u):
        return (n * u).astype(np.float32)

    def compute_flux_u(self, u):
        return (0.5 * u * u).astype(np.float32)

    def laplacian_u(self, u):
        """Second derivative with periodic BC."""
        return (np.roll(u, -1) - 2*u + np.roll(u, 1)) / (self.dx**2)

    def step(self, state):
        n, u, E = state

        # continuity
        F_n = self.compute_flux_n(n, u)
        F_n_left = np.roll(F_n, 1)
        n_new = n - (self.dt/self.dx) * (F_n - F_n_left)

        # momentum with advection + E + viscosity
        F_u = self.compute_flux_u(u)
        F_u_left = np.roll(F_u, 1)
        u_adv = u - (self.dt/self.dx) * (F_u - F_u_left)

        lap_u = self.laplacian_u(u)
        u_new = u_adv + self.dt * (E + self.nu * lap_u)

        E_new = self.solve_poisson(n_new)
        state_new = np.stack([n_new, u_new, E_new], axis=0).astype(np.float32)
        return state_new, F_n

    def run(self, state0, n_steps=10, record_flux=True):
        states = [state0.astype(np.float32)]
        fluxes = []
        state = state0.astype(np.float32)
        for _ in range(n_steps):
            state, F_n = self.step(state)
            states.append(state)
            if record_flux:
                fluxes.append(F_n)
        states = np.stack(states, axis=0)
        if record_flux:
            fluxes = np.stack(fluxes, axis=0)
        else:
            fluxes = None
        return states, fluxes

print("BaselineSolver smoke test...")
solver = BaselineSolver()
s0 = solver.initial_condition(seed=0)
states_smoke, fluxes_smoke = solver.run(s0, n_steps=5)
print(" states:", states_smoke.shape, " fluxes:", fluxes_smoke.shape)

# @title Cell 3 Generate dataset (state_t, flux_t, state_{t+1})

# def generate_dataset(
#     nx=64,
#     num_initial_conditions=4,   # increase to 8, 16 later to stress generalization
#     steps_per_ic=30,            # increase to 40–80 for longer horizons
#     dt=5e-3,
#     t_end=1.0,
# ):
#     solver = BaselineSolver(nx=nx, dt=dt, t_end=t_end)
#     all_state_t, all_flux_t, all_state_next = [], [], []

#     for ic in range(num_initial_conditions):
#         state0 = solver.initial_condition(seed=ic)
#         states, fluxes = solver.run(state0, n_steps=steps_per_ic, record_flux=True)
#         all_state_t.append(states[:-1])      # [steps, 3, nx]
#         all_flux_t.append(fluxes)           # [steps, nx]
#         all_state_next.append(states[1:])   # [steps, 3, nx]

#     state_t = np.concatenate(all_state_t, axis=0)       # [N, 3, nx]
#     flux_t = np.concatenate(all_flux_t, axis=0)         # [N, nx]
#     state_next = np.concatenate(all_state_next, axis=0) # [N, 3, nx]
#     x = solver.x.astype(np.float32)

#     print("Dataset shapes:")
#     print(" state_t   :", state_t.shape)
#     print(" flux_t    :", flux_t.shape)
#     print(" state_next:", state_next.shape)
#     return state_t, flux_t, state_next, x, dt, solver.dx

# state_t, flux_t, state_next, x_all, dt_sim, dx_sim = generate_dataset()


####################################################################


def generate_dataset(
    nx=64,
    num_initial_conditions=20, ### initially was 4
    steps_per_ic=30,
    dt=5e-3,
    t_end=1.0,
    nu=1e-3,
):
    solver = BaselineSolver(nx=nx, dt=dt, t_end=t_end, nu=nu)
    all_state_t, all_flux_t, all_state_next = [], [], []
    for ic in range(num_initial_conditions):
        state0 = solver.initial_condition(seed=ic)
        states, fluxes = solver.run(state0, n_steps=steps_per_ic, record_flux=True)
        all_state_t.append(states[:-1])
        all_flux_t.append(fluxes)
        all_state_next.append(states[1:])
    state_t = np.concatenate(all_state_t, axis=0)
    flux_t = np.concatenate(all_flux_t, axis=0)
    state_next = np.concatenate(all_state_next, axis=0)
    x = solver.x.astype(np.float32)
    print("Dataset shapes:")
    print(" state_t   :", state_t.shape)
    print(" flux_t    :", flux_t.shape)
    print(" state_next:", state_next.shape)
    return state_t, flux_t, state_next, x, dt, solver.dx, solver.nu

state_t, flux_t, state_next, x_all, dt_sim, dx_sim, nu_sim = generate_dataset()

"""
* **Richer initial conditions**

  * More modes (3–5 sinusoidal modes in density).
  * Larger amplitudes (up to (~ 0.3)).
  * Velocity is a mix of modes + small noise.
* **Viscous momentum equation**

We now use:

* **Continuity:**
  [ $$
  \frac{\partial n}{\partial t} ;+; \frac{\partial (n u)}{\partial x} ;=; 0 $$
  ]

* **Momentum with advection and viscosity:**
  [ $$
  \frac{\partial u}{\partial t}
  ;+;
  \frac{\partial}{\partial x}!\left(\frac{u^2}{2}\right)
  ;=;
  E ;+;
  \nu,\frac{\partial^2 u}{\partial x^2} $$
  ]

* **Poisson:**
  [ $$
  \frac{\partial E}{\partial x}
  ;=;
  n - n_0,
  \quad
  n_0 = 1 $$
  ]

Viscosity term is implemented as a discrete Laplacian with periodic BC.
"""

# @title Cell 4 Graph constructor (features = [n, u, E, x])

# def build_chain_graph(full_state, x):
#     """
#     full_state: [3, nx] -> (n, u, E)
#     x: [nx]
#     Returns:
#       node_features [nx, 4] -> [n, u, E, x]
#       edge_index   [2, 2*nx] (periodic i <-> i+1)
#     """
#     n = full_state[0].astype(np.float32)
#     u = full_state[1].astype(np.float32)
#     E = full_state[2].astype(np.float32)
#     x = x.astype(np.float32)
#     nx = n.shape[0]

#     node_features = torch.from_numpy(
#         np.stack([n, u, E, x], axis=-1)
#     )  # [nx, 4]

#     src = np.arange(nx, dtype=np.int64)
#     dst = (src + 1) % nx
#     src_all = np.concatenate([src, dst], axis=0)
#     dst_all = np.concatenate([dst, src], axis=0)
#     edge_index = torch.from_numpy(np.stack([src_all, dst_all], axis=0))
#     return node_features, edge_index

# nf_test, ei_test = build_chain_graph(state_t[0], x_all)
# print("Graph test: node_features", nf_test.shape, "edge_index", ei_test.shape)



def build_chain_graph(full_state, x, device=None):
    """
    full_state: [3, nx] torch or numpy
    x: numpy [nx]
    Returns:
      node_features [nx,4] torch
      edge_index   [2,2*nx] torch.long
    """
    if isinstance(full_state, np.ndarray):
        n = torch.from_numpy(full_state[0].astype(np.float32))
        u = torch.from_numpy(full_state[1].astype(np.float32))
        E = torch.from_numpy(full_state[2].astype(np.float32))
    else:
        n = full_state[0]
        u = full_state[1]
        E = full_state[2]

    if device is None:
        device = n.device
    else:
        n = n.to(device)
        u = u.to(device)
        E = E.to(device)

    x_t = torch.as_tensor(x, dtype=torch.float32, device=device)
    nx = n.shape[0]
    node_features = torch.stack([n, u, E, x_t], dim=-1)

    src = torch.arange(nx, dtype=torch.long, device=device)
    dst = (src + 1) % nx
    src_all = torch.cat([src, dst], dim=0)
    dst_all = torch.cat([dst, src], dim=0)
    edge_index = torch.stack([src_all, dst_all], dim=0)
    return node_features, edge_index

nf_test, ei_test = build_chain_graph(state_t[0], x_all, device=device)
print("Graph test:", nf_test.shape, ei_test.shape)

# @title Cell 5 FluxGNN model (deeper, wider)

# class FluxGNN(nn.Module):
#     def __init__(self, in_features=4, hidden_dim=64, num_layers=3):
#         super().__init__()
#         self.input_mlp = nn.Sequential(
#             nn.Linear(in_features, hidden_dim),
#             nn.ReLU(),
#         )
#         self.update_mlps = nn.ModuleList(
#             [
#                 nn.Sequential(
#                     nn.Linear(2 * hidden_dim, hidden_dim),
#                     nn.ReLU(),
#                 )
#                 for _ in range(num_layers)
#             ]
#         )
#         self.edge_mlp = nn.Sequential(
#             nn.Linear(2 * hidden_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, 1),
#         )

#     def forward(self, node_features, edge_index):
#         h = self.input_mlp(node_features)
#         row, col = edge_index
#         N = h.size(0)

#         for mlp in self.update_mlps:
#             agg = torch.zeros_like(h)
#             agg.index_add_(0, row, h[col])
#             deg = torch.bincount(row, minlength=N).clamp_min(1).float().unsqueeze(-1)
#             agg = agg / deg
#             h = mlp(torch.cat([h, agg], dim=-1))

#         h_row = h[row]
#         h_col = h[col]
#         edge_feat = torch.cat([h_row, h_col], dim=-1)
#         flux = self.edge_mlp(edge_feat).squeeze(-1)
#         return flux

# # smoke test
# gnn_test = FluxGNN()
# out_test = gnn_test(nf_test.float(), ei_test.long())
# print("FluxGNN smoke output:", out_test.shape)



###########################################################################


class FluxGNN(nn.Module):
    def __init__(self, in_features=4, hidden_dim=64, num_layers=3):
        super().__init__()
        self.input_mlp = nn.Sequential(
            nn.Linear(in_features, hidden_dim),
            nn.ReLU(),
        )
        self.update_mlps = nn.ModuleList(
            [
                nn.Sequential(
                    nn.Linear(2*hidden_dim, hidden_dim),
                    nn.ReLU(),
                )
                for _ in range(num_layers)
            ]
        )
        self.edge_mlp = nn.Sequential(
            nn.Linear(2*hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
        )

    def forward(self, node_features, edge_index):
        h = self.input_mlp(node_features)
        row, col = edge_index
        N = h.size(0)
        for mlp in self.update_mlps:
            agg = torch.zeros_like(h)
            agg.index_add_(0, row, h[col])
            deg = torch.bincount(row, minlength=N).clamp_min(1).float().unsqueeze(-1)
            agg = agg / deg
            h = mlp(torch.cat([h, agg], dim=-1))
        h_row = h[row]
        h_col = h[col]
        edge_feat = torch.cat([h_row, h_col], dim=-1)
        flux = self.edge_mlp(edge_feat).squeeze(-1)
        return flux

gnn_smoke = FluxGNN().to(device)
out_smoke = gnn_smoke(nf_test, ei_test)
print("FluxGNN smoke output:", out_smoke.shape)
# print('happy')

"""# physics losses

In the training loop we now have:

1. **Flux supervision:** MSE between predicted and true continuity flux.
2. **One-step density consistency:** predicted
$$ ( n_{t+1}^{\text{pred}} ) $$ vs true $$ ( n_{t+1} ).$$
3. **Poisson consistency:**
$$( E(n_{t+1}^{\text{pred}}) ) $$ vs true $$ ( E_{t+1} ).$$

4. **Explicit charge conservation vs t:**
   • Penalize difference between total charge at t and at t+1 *for the predicted state*:
 [ $$
   \left( \sum (n_t - n_0), dx ;-; \sum (n_{t+1}^{\text{pred}} - n_0), dx \right)^2
   $$ ]
5. **One-step “energy-like” consistency:** match a simple energy functional at t+1.
6. **Multi-step energy conservation:**
   • Roll out the hybrid (using the GNN flux) for `rollout_steps` steps (e.g., 3).
   • Compute energy at each step (using (u^2); gradient flows through (u)).
   • Penalize deviations from the energy at the first step:
   [
 $$  \frac{1}{K} \sum_k (E_k - E_0)^2 . $$
   ]

So now energy is encouraged to be consistent across **multiple hybrid steps**, not just one.

"""

# @title Cell 6 Dataset wrapper + physics-informed training

# class FluxDataset(Dataset):
#     """
#     Each sample:
#       - state_t    [3, nx]
#       - flux_t     [nx]
#       - state_next [3, nx]
#     """
#     def __init__(self, state_t, flux_t, state_next):
#         assert state_t.shape == state_next.shape
#         assert state_t.shape[0] == flux_t.shape[0]
#         self.state_t = state_t
#         self.flux_t = flux_t
#         self.state_next = state_next
#         self.N, self.C, self.nx = state_t.shape

#     def __len__(self):
#         return self.N

#     def __getitem__(self, idx):
#         return (
#             self.state_t[idx],
#             self.flux_t[idx],
#             self.state_next[idx],
#         )

# def collate_fn(batch):
#     # single-sample batches for clarity
#     return batch[0]

# def solve_poisson_np(n_np, n0, dx):
#     """Numpy Poisson solve for physics loss."""
#     nx = n_np.shape[0]
#     rho = n_np - n0
#     k = 2.0 * np.pi * np.fft.fftfreq(nx, d=dx)
#     rho_hat = np.fft.fft(rho)
#     E_hat = np.zeros_like(rho_hat, dtype=complex)
#     mask = k != 0
#     E_hat[mask] = 1j * rho_hat[mask] / k[mask]
#     E_hat[~mask] = 0.0
#     E = np.real(np.fft.ifft(E_hat)).astype(np.float32)
#     return E

# def train_model(
#     state_t,
#     flux_t,
#     state_next,
#     x,
#     dt,
#     dx,
#     epochs=100,
#     lr=1e-3,
#     hidden_dim=64,
#     num_layers=3,
#     device=device,
#     lambda_state=1.0,
#     lambda_poisson=0.1,
#     lambda_charge=0.1,
#     lambda_energy=0.05,
# ):
#     dataset = FluxDataset(state_t, flux_t, state_next)
#     loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)

#     model = FluxGNN(in_features=4, hidden_dim=hidden_dim, num_layers=num_layers).to(device)
#     optimizer = optim.Adam(model.parameters(), lr=lr)
#     mse = nn.MSELoss()
#     n0 = 1.0

#     print("Training...")
#     model.train()
#     for epoch in range(1, epochs+1):
#         total_loss = 0.0
#         for st_np, ft_np, st_next_np in loader:
#             # convert to torch
#             st = torch.from_numpy(st_np).float().to(device)       # [3, nx]
#             ft_true = torch.from_numpy(ft_np).float().to(device)  # [nx]
#             st_next_true = torch.from_numpy(st_next_np).float().to(device)

#             n_t = st[0]
#             u_t = st[1]
#             E_t = st[2]
#             n_next_true = st_next_true[0]
#             u_next_true = st_next_true[1]
#             E_next_true = st_next_true[2]

#             # Graph from state_t
#             node_features, edge_index = build_chain_graph(st_np, x)
#             node_features = node_features.to(device)
#             edge_index = edge_index.to(device)

#             # GNN flux prediction (edge level -> cell level)
#             flux_edge = model(node_features, edge_index)   # [2*nx]
#             nx_loc = n_t.shape[0]
#             F_forward = flux_edge[:nx_loc]
#             F_backward = flux_edge[nx_loc:]
#             F_pred = 0.5 * (F_forward + F_backward)        # [nx]

#             # 1) flux supervision
#             flux_loss = mse(F_pred, ft_true)

#             # 2) one-step density with predicted flux
#             F_left_pred = torch.roll(F_pred, 1)
#             n_next_pred = n_t - (dt/dx) * (F_pred - F_left_pred)
#             state_loss = mse(n_next_pred, n_next_true)

#             # 3) Poisson consistency (numpy-based)
#             n_next_pred_np = n_next_pred.detach().cpu().numpy()
#             E_next_pred_np = solve_poisson_np(n_next_pred_np, n0, dx)
#             E_next_pred = torch.from_numpy(E_next_pred_np).to(device)
#             poisson_loss = mse(E_next_pred, E_next_true)

#             # 4) global charge conservation vs true
#             charge_pred = torch.sum(n_next_pred - n0) * dx
#             charge_true = torch.sum(n_next_true - n0) * dx
#             charge_loss = mse(charge_pred, charge_true)

#             # 5) energy-like consistency (u^2 + E^2)
#             energy_pred = 0.5 * torch.mean(u_next_true**2 + E_next_pred**2)
#             energy_true = 0.5 * torch.mean(u_next_true**2 + E_next_true**2)
#             energy_loss = mse(energy_pred, energy_true)

#             loss = (
#                 flux_loss
#                 + lambda_state * state_loss
#                 + lambda_poisson * poisson_loss
#                 + lambda_charge * charge_loss
#                 + lambda_energy * energy_loss
#             )

#             optimizer.zero_grad()
#             loss.backward()
#             optimizer.step()
#             total_loss += loss.item()

#         avg_loss = total_loss / len(loader)
#         print(f"[Train] Epoch {epoch}/{epochs} - Loss: {avg_loss:.6e}")

#     return model

# model_trained = train_model(
#     state_t,
#     flux_t,
#     state_next,
#     x_all,
#     dt_sim,
#     dx_sim,
#     epochs=100,
# )



############################################################################

class FluxDataset(Dataset):
    def __init__(self, state_t, flux_t, state_next):
        assert state_t.shape == state_next.shape
        assert state_t.shape[0] == flux_t.shape[0]
        self.state_t = state_t
        self.flux_t = flux_t
        self.state_next = state_next
        self.N, self.C, self.nx = state_t.shape

    def __len__(self):
        return self.N

    def __getitem__(self, idx):
        return (
            self.state_t[idx],
            self.flux_t[idx],
            self.state_next[idx],
        )

def collate_fn(batch):
    return batch[0]

def solve_poisson_np(n_np, n0, dx):
    nx = n_np.shape[0]
    rho = n_np - n0
    k = 2.0 * np.pi * np.fft.fftfreq(nx, d=dx)
    rho_hat = np.fft.fft(rho)
    E_hat = np.zeros_like(rho_hat, dtype=complex)
    mask = k != 0
    E_hat[mask] = 1j * rho_hat[mask] / k[mask]
    E_hat[~mask] = 0.0
    E = np.real(np.fft.ifft(E_hat)).astype(np.float32)
    return E

def train_model(
    state_t,
    flux_t,
    state_next,
    x,
    dt,
    dx,
    nu,
    epochs=20,
    lr=1e-3,
    hidden_dim=64,
    num_layers=3,
    device=device,
    lambda_state=1.0,
    lambda_poisson=0.1,
    lambda_charge=0.1,
    lambda_energy_one=0.05,
    lambda_energy_multi=0.05,
    rollout_steps=3,
):
    dataset = FluxDataset(state_t, flux_t, state_next)
    loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)
    model = FluxGNN(in_features=4, hidden_dim=hidden_dim, num_layers=num_layers).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()
    n0 = 1.0

    print("Training...")
    model.train()
    for epoch in range(1, epochs+1):
        total_loss = 0.0
        for st_np, ft_np, st_next_np in loader:
            st = torch.from_numpy(st_np).float().to(device)
            ft_true = torch.from_numpy(ft_np).float().to(device)
            st_next_true = torch.from_numpy(st_next_np).float().to(device)

            n_t = st[0]
            u_t = st[1]
            E_t = st[2]
            n_next_true = st_next_true[0]
            u_next_true = st_next_true[1]
            E_next_true = st_next_true[2]

            # --- flux at time t ---
            node_features, edge_index = build_chain_graph(st, x, device=device)
            flux_edge = model(node_features, edge_index)
            nx_loc = n_t.shape[0]
            F_forward = flux_edge[:nx_loc]
            F_backward = flux_edge[nx_loc:]
            F_pred = 0.5 * (F_forward + F_backward)

            # 1) flux loss
            flux_loss = mse(F_pred, ft_true)

            # 2) one-step density update
            F_left_pred = torch.roll(F_pred, 1)
            n_next_pred = n_t - (dt/dx) * (F_pred - F_left_pred)
            state_loss = mse(n_next_pred, n_next_true)

            # 3) Poisson consistency
            n_next_pred_np = n_next_pred.detach().cpu().numpy()
            E_next_pred_np = solve_poisson_np(n_next_pred_np, n0, dx)
            E_next_pred = torch.from_numpy(E_next_pred_np).to(device)
            poisson_loss = mse(E_next_pred, E_next_true)

            # 4) explicit charge conservation vs t
            charge_t = torch.sum(n_t - n0) * dx
            charge_next_pred = torch.sum(n_next_pred - n0) * dx
            charge_loss = mse(charge_next_pred, charge_t)

            # 5) one-step energy-like consistency vs true at t+1
            energy_next_pred = 0.5 * torch.mean(u_next_true**2 + E_next_pred**2)
            energy_next_true = 0.5 * torch.mean(u_next_true**2 + E_next_true**2)
            energy_one_loss = mse(energy_next_pred, energy_next_true)

            # 6) multi-step energy conservation over rollout_steps
            state_roll = st.clone()
            energies = []
            for k in range(rollout_steps):
                n_r = state_roll[0]
                u_r = state_roll[1]
                E_r = state_roll[2]

                # energy at step k (gradient via u_r; E_r treated as constant)
                energy_k = 0.5 * torch.mean(u_r**2)
                energies.append(energy_k)

                node_f_r, edge_idx_r = build_chain_graph(state_roll, x, device=device)
                flux_edge_r = model(node_f_r, edge_idx_r)
                F_fwd_r = flux_edge_r[:nx_loc]
                F_bwd_r = flux_edge_r[nx_loc:]
                F_r = 0.5 * (F_fwd_r + F_bwd_r)

                F_left_r = torch.roll(F_r, 1)
                n_next_r = n_r - (dt/dx) * (F_r - F_left_r)

                F_u_r = 0.5 * u_r * u_r
                F_u_left_r = torch.roll(F_u_r, 1)
                u_adv_r = u_r - (dt/dx) * (F_u_r - F_u_left_r)
                u_next_r = u_adv_r + dt * E_r

                n_next_r_np = n_next_r.detach().cpu().numpy()
                E_next_r_np = solve_poisson_np(n_next_r_np, n0, dx)
                E_next_r = torch.from_numpy(E_next_r_np).to(device)

                state_roll = torch.stack([n_next_r, u_next_r, E_next_r], dim=0)

            energies = torch.stack(energies)
            energy_multi_loss = torch.mean((energies - energies[0])**2)

            loss = (
                flux_loss
                + lambda_state * state_loss
                + lambda_poisson * poisson_loss
                + lambda_charge * charge_loss
                + lambda_energy_one * energy_one_loss
                + lambda_energy_multi * energy_multi_loss
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(loader)
        print(f"[Train] Epoch {epoch}/{epochs} - Loss: {avg_loss:.6e}")

    return model

start_time = time.time()
model_trained = train_model(
    state_t,
    flux_t,
    state_next,
    x_all,
    dt_sim,
    dx_sim,
    nu_sim,
    epochs=20,
)
print("Training time (s):", time.time() - start_time)

# @title Cell 7 Hybrid solver + evaluation and plots

class HybridSolver:
    """
    Hybrid solver:
      - continuity flux from trained GNN
      - velocity update and Poisson solve from baseline
    """
    def __init__(self, model, nx=64, length=2*math.pi, dt=5e-3, t_end=1.0, device=device):
        self.baseline = BaselineSolver(nx=nx, length=length, dt=dt, t_end=t_end)
        self.model = model.to(device)
        self.model.eval()
        self.device = device

    def step(self, state):
        n, u, E = state

        # GNN flux
        node_features, edge_index = build_chain_graph(state, self.baseline.x)
        node_features = node_features.to(self.device)
        edge_index = edge_index.to(self.device)
        with torch.no_grad():
            flux_edge = self.model(node_features, edge_index).cpu().numpy()
        nx = self.baseline.nx
        F_forward = flux_edge[:nx]
        F_backward = flux_edge[nx:]
        F_pred = 0.5 * (F_forward + F_backward).astype(np.float32)

        # continuity update
        F_left = np.roll(F_pred, 1)
        n_new = n - (self.baseline.dt/self.baseline.dx) * (F_pred - F_left)

        # velocity update: same physics as baseline
        F_u = 0.5 * u * u
        F_u_left = np.roll(F_u, 1)
        u_adv = u - (self.baseline.dt/self.baseline.dx) * (F_u - F_u_left)
        u_new = u_adv + self.baseline.dt * E

        # Poisson solve
        E_new = self.baseline.solve_poisson(n_new)

        state_new = np.stack([n_new, u_new, E_new], axis=0).astype(np.float32)
        return state_new

    def run(self, state0, n_steps=40):
        states = [state0.astype(np.float32)]
        state = state0.astype(np.float32)
        for _ in range(n_steps):
            state = self.step(state)
            states.append(state)
        states = np.stack(states, axis=0)   # [T+1, 3, nx]
        return states

print("Evaluating baseline vs hybrid...")

nx_eval = 64
dt_eval = dt_sim
t_end_eval = 1.0
n_steps_eval = 50

baseline_eval = BaselineSolver(nx=nx_eval, dt=dt_eval, t_end=t_end_eval)
state0_eval = baseline_eval.initial_condition(seed=123)
states_b, _ = baseline_eval.run(state0_eval, n_steps=n_steps_eval)

hybrid_eval = HybridSolver(model_trained, nx=nx_eval, dt=dt_eval, t_end=t_end_eval)
states_h = hybrid_eval.run(state0_eval, n_steps=n_steps_eval)

# density trajectories
n_b = states_b[:, 0, :]   # [T+1, nx]
n_h = states_h[:, 0, :]

mse_traj = np.mean((n_b - n_h)**2)
print("Trajectory MSE (density n):", mse_traj)

# MSE vs time
mse_t = np.mean((n_b - n_h)**2, axis=1)
x = baseline_eval.x
t_grid = np.linspace(0, n_steps_eval*dt_eval, n_steps_eval+1)

# 1) Final density comparison
plt.figure(figsize=(6,4))
plt.plot(x, n_b[-1], label="Baseline n(T)", linewidth=2)
plt.plot(x, n_h[-1], label="Hybrid n(T)", linestyle="--")
plt.xlabel("x")
plt.ylabel("n(x, T)")
plt.title("Baseline vs Hybrid density at final time")
plt.legend()
plt.tight_layout()
plt.show()

# 2) Space–time heatmaps
fig, axes = plt.subplots(1, 2, figsize=(10,4), sharey=True)
im0 = axes[0].imshow(n_b, aspect='auto', origin='lower',
                     extent=[x[0], x[-1], 0, n_steps_eval*dt_eval])
axes[0].set_title("Baseline n(x,t)")
axes[0].set_xlabel("x")
axes[0].set_ylabel("t")
fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)

im1 = axes[1].imshow(n_h, aspect='auto', origin='lower',
                     extent=[x[0], x[-1], 0, n_steps_eval*dt_eval])
axes[1].set_title("Hybrid n(x,t)")
axes[1].set_xlabel("x")
fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)

plt.tight_layout()
plt.show()

# 3) MSE vs time
plt.figure(figsize=(6,4))
plt.plot(t_grid, mse_t, marker='o')
plt.xlabel("time")
plt.ylabel("MSE(n_baseline, n_hybrid)")
plt.title("Density MSE over time")
plt.tight_layout()
plt.show()

"""# Previous work to compare our results with:

(A) Flux-learning / hybrid integrator idea

Kim & Kang (2024) – Fourier Neural Operators as learned fluxes,

Taeyoung Kim, Myungjoo Kang, “Approximating Numerical Fluxes Using Fourier Neural Operators for Hyperbolic Conservation Laws” https://arxiv.org/abs/2401.01783

They also:

keep a classical time integrator,

but replace the numerical flux with a learned operator (FNO),

and design flux-level losses derived from conservation-law schemes.

This is very close in spirit to what you’re doing (GNN-learned flux + finite-volume integrator), even though:

their PDEs are generic hyperbolic conservation laws (Burgers, shallow water, etc.),

your PDE is fluid–Poisson with plasma physics structure.

How to compare:

Long-time stability and error growth vs. number of time steps.

Resolution invariance (can your GNN flux generalize to different grid sizes the way their FNO flux does?).

Out-of-distribution initial conditions (larger amplitudes, different spectra).

You don’t have to replicate their exact experiments; you can position your work as:

“GNN analogue of Kim & Kang’s flux-FNO approach, but tested on a 1D plasma fluid–Poisson system rather than simpler scalar conservation laws.”




--------------------------------------


Carvalho et al. (2024) – fully learned GNN surrogate for 1D plasma model

Diogo D. Carvalho, Diogo R. Ferreira, Luís O. Silva,
“Learning the dynamics of a one-dimensional plasma model with graph neural networks,” Machine Learning: Science and Technology 5, https://arxiv.org/abs/2310.17646

They:

use a kinetic 1D plasma model (Vlasov-type, “one-dimensional plasma model”),

build a GNN that fully replaces the kinetic solver,

carefully check conservation laws, spectra, and known plasma processes (Landau damping, thermalization, etc.).

Your angle:

You do not replace the entire solver; you only learn the fluid flux operator and plug it into a classical time integrator.

You work with a fluid reduction rather than full kinetic PIC / sheets.

How to compare:

Conservation properties: charge and an energy-like quantity; error vs time.

Ability to capture frequency and damping rate of plasma oscillations.

Sample efficiency: how much data do you need compared to a fully learned GNN surrogate?

Framing line for the paper:

“Relative to Carvalho et al., who train a GNN to emulate a full kinetic 1D plasma solver, we focus on a cheaper fluid–Poisson system and target only the numerical flux, keeping the rest of the finite-volume scheme explicit. This lets us study when ‘just learning the flux’ is sufficient to recover accurate, stable plasma dynamics.”

-----------------------------------


Mlinarević et al. (2025) – GNN surrogate for PIC

Marin Mlinarević, George K. Holt, Adriano Agnello,
“Particle-based plasma simulation using a graph neural network,” https://arxiv.org/abs/2503.00274

They:

build a GNN surrogate for particle-in-cell (PIC) simulations,

represent particles and fields on a graph,

reproduce two-stream instabilities and other kinetic effects.

How to compare:

Conceptual positioning: they are “GNN surrogate for PIC,” you are “GNN flux surrogate for fluid–Poisson.”

If you later extend to more realistic ICs (e.g., multi-mode perturbations, instability-like behaviour), you can qualitatively compare your ability to reproduce instability growth rates and saturation behaviour at the fluid level.

-------------------------------------

Vigot et al. (2025) – GNN for Poisson equation in plasma simulations

G. Vigot et al., “Graph neural networks for computational plasma physics on unstructured grids: application to approximate the Poisson equation for Hall-effect thrusters modeling.”
https://link.springer.com/content/pdf/10.1007/s44205-025-00146-w.pdf

They:

use a GNN to approximate the solution of the Poisson equation on an unstructured grid in a 3D plasma simulation,

again in a hybrid spirit: classical PDE model, but replace an expensive component with a GNN.

How to compare:

Another example of hybrid architectures where a neural module replaces part of the solver (Poisson solve vs. flux evaluation).

You can benchmark conceptual advantages: your approach handles the hyperbolic transport part, theirs the elliptic field solve.

------------------------------------
for future work:
Beyond the plasma-specific works, you can also briefly reference broader hybrid-PDE papers that echo your design choice of “classic integrator + learned local operator”:

“Neural Operators Learn the Local Physics of Magnetohydrodynamics (MHD)” – combines numerical MHD solvers with neural operators to reduce cost and enforce physical structure.
https://arxiv.org/html/2404.16015v1

This helps you argue:

“Our work follows the emerging trend of hybrid solvers where neural networks learn localized operators (fluxes, closures, Poisson solves) while classical schemes provide stable time integration.”
"""

